{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMe+bk9MpYv+IewQt84yiXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rizkii-dev/Paraphrasing/blob/main/Copy_of_Paraphrasing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPFxfwt7hpq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66abcb20"
      },
      "source": [
        "# Task\n",
        "Develop an AI paraphraser that transforms English sentences into clearer and more formal versions, similar to how \"Students must submit the report before Friday\" might be rephrased. This will involve selecting and fine-tuning a pre-trained language model using example sentence pairs to adapt it to this specific paraphrasing style, and then implementing and demonstrating a function to perform the transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ca4890d"
      },
      "source": [
        "## Understand Paraphrasing Requirements\n",
        "\n",
        "### Subtask:\n",
        "Clarify the specific rules and transformations required for making sentences clearer and more formal, based on the provided example and any additional guidelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12d0470d"
      },
      "source": [
        "### Subtask\n",
        "Clarify the specific rules and transformations required for making sentences clearer and more formal, based on the provided example and any additional guidelines.\n",
        "\n",
        "#### Instructions\n",
        "1. **Analyze the example**: \"Students must submit the report before Friday\". Consider how a \"clearer and more formal\" version might differ. For instance, what alternative phrasing for \"must submit\" could convey formality? How might the sentence structure change?\n",
        "2. **Brainstorm common patterns for formal language in English**. Think about common informal words or phrases and their more formal equivalents (e.g., 'get' vs. 'obtain', 'need to' vs. 'it is necessary to'). Consider sentence structure, such as avoiding contractions, using passive voice where appropriate for objectivity, and employing more sophisticated vocabulary.\n",
        "3. **Identify characteristics that make a sentence 'clearer'**. This might involve breaking down complex sentences, ensuring precise word choice, or rephrasing ambiguous statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d283c47"
      },
      "source": [
        "### Subtask\n",
        "Clarify the specific rules and transformations required for making sentences clearer and more formal, based on the provided example and any additional guidelines.\n",
        "\n",
        "#### Instructions\n",
        "1. **Analyze the example**: \"Students must submit the report before Friday\". Consider how a \"clearer and more formal\" version might differ. For instance, what alternative phrasing for \"must submit\" could convey formality? How might the sentence structure change?\n",
        "2. **Brainstorm common patterns for formal language in English**. Think about common informal words or phrases and their more formal equivalents (e.g., 'get' vs. 'obtain', 'need to' vs. 'it is necessary to'). Consider sentence structure, such as avoiding contractions, using passive voice where appropriate for objectivity, and employing more sophisticated vocabulary.\n",
        "3. **Identify characteristics that make a sentence 'clearer'**. This might involve breaking down complex sentences, ensuring precise word choice, or rephrasing ambiguous statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53b7b13e"
      },
      "source": [
        "### Subtask\n",
        "Clarify the specific rules and transformations required for making sentences clearer and more formal, based on the provided example and any additional guidelines.\n",
        "\n",
        "#### Instructions\n",
        "1. **Analyze the example**: \"Students must submit the report before Friday\". Consider how a \"clearer and more formal\" version might differ. For instance, what alternative phrasing for \"must submit\" could convey formality? How might the sentence structure change?\n",
        "2. **Brainstorm common patterns for formal language in English**. Think about common informal words or phrases and their more formal equivalents (e.g., 'get' vs. 'obtain', 'need to' vs. 'it is necessary to'). Consider sentence structure, such as avoiding contractions, using passive voice where appropriate for objectivity, and employing more sophisticated vocabulary.\n",
        "3. **Identify characteristics that make a sentence 'clearer'**. This might involve breaking down complex sentences, ensuring precise word choice, or rephrasing ambiguous statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97b0799"
      },
      "source": [
        "## Prepare Example Sentences\n",
        "\n",
        "### Subtask:\n",
        "Gather a dataset of example sentences, including both their original (less clear/formal) and desired (clearer/more formal) versions, to train or fine-tune the paraphraser.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d057c478"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "1.  **Brainstorm or find a collection of short to medium length English sentences that are informal or could be made clearer and more formal.**\n",
        "    *   Consider common phrases used in everyday communication versus professional or academic settings.\n",
        "    *   *Self-Correction*: I will manually create 5-10 such pairs to serve as initial examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397e59ed"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "1.  **Brainstorm or find a collection of short to medium length English sentences that are informal or could be made clearer and more formal.**\n",
        "    *   Consider common phrases used in everyday communication versus professional or academic settings.\n",
        "    *   *Self-Correction*: I will manually create 5-10 such pairs to serve as initial examples.\n",
        "2.  **For each original informal sentence, manually rephrase it to create a corresponding clearer and more formal version.** Focus on the transformation rules identified in the previous step (e.g., replacing informal vocabulary, avoiding contractions, improving sentence structure, ensuring precise word choice)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf97f605"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "1.  **Brainstorm or find a collection of short to medium length English sentences that are informal or could be made clearer and more formal.**\n",
        "    *   Consider common phrases used in everyday communication versus professional or academic settings.\n",
        "    *   *Self-Correction*: I will manually create 5-10 such pairs to serve as initial examples.\n",
        "2.  **For each original informal sentence, manually rephrase it to create a corresponding clearer and more formal version.** Focus on the transformation rules identified in the previous step (e.g., replacing informal vocabulary, avoiding contractions, improving sentence structure, ensuring precise word choice).\n",
        "3.  **Organize these sentence pairs into a structured format**, such as a list of dictionaries or a pandas DataFrame, where each entry contains an 'original' sentence and its 'formal' counterpart. Aim for at least 5-10 diverse sentence pairs to start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4d02bea"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "1.  **Brainstorm or find a collection of short to medium length English sentences that are informal or could be made clearer and more formal.**\n",
        "    *   Consider common phrases used in everyday communication versus professional or academic settings.\n",
        "    *   *Self-Correction*: I will manually create 5-10 such pairs to serve as initial examples.\n",
        "2.  **For each original informal sentence, manually rephrase it to create a corresponding clearer and more formal version.** Focus on the transformation rules identified in the previous step (e.g., replacing informal vocabulary, avoiding contractions, improving sentence structure, ensuring precise word choice).\n",
        "3.  **Organize these sentence pairs into a structured format**, such as a list of dictionaries or a pandas DataFrame, where each entry contains an 'original' sentence and its 'formal' counterpart. Aim for at least 5-10 diverse sentence pairs to start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418191d8"
      },
      "source": [
        "### Creating Example Sentence Pairs\n",
        "\n",
        "As per the instructions, I will now create 5-10 pairs of original (informal/less clear) and formal (clearer/more formal) sentences. These pairs will be stored in a pandas DataFrame for structured use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85adef43"
      },
      "source": [
        "### Creating Example Sentence Pairs\n",
        "\n",
        "As per the instructions, I will now create 5-10 pairs of original (informal/less clear) and formal (clearer/more formal) sentences. These pairs will be stored in a pandas DataFrame for structured use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdd2d06b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now define the example sentence pairs and store them in a pandas DataFrame, as outlined in the previous markdown block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca76633d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "sentence_pairs = [\n",
        "    {\n",
        "        \"original\": \"I gotta go now, fam.\",\n",
        "        \"formal\": \"I must depart at this moment.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"Can you gimme a hand with this project?\",\n",
        "        \"formal\": \"Could you please assist me with this project?\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"The boss wants us to get this done ASAP.\",\n",
        "        \"formal\": \"The supervisor requires this task to be completed as soon as possible.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"It's kinda hard to understand what you mean.\",\n",
        "        \"formal\": \"It is somewhat difficult to comprehend your intended meaning.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"That's a pretty cool idea.\",\n",
        "        \"formal\": \"That is a rather innovative concept.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"We need to fix this problem, like, right away.\",\n",
        "        \"formal\": \"It is imperative that we resolve this issue immediately.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"He's always messin' things up.\",\n",
        "        \"formal\": \"He consistently causes complications.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"I'm not gonna make it to the meeting today.\",\n",
        "        \"formal\": \"I will be unable to attend today's meeting.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"The situation is pretty bad, to be honest.\",\n",
        "        \"formal\": \"The current circumstances are quite unfavorable, to be frank.\"\n",
        "    },\n",
        "    {\n",
        "        \"original\": \"Lemme know if you need anything.\",\n",
        "        \"formal\": \"Please inform me if you require any assistance.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "df_sentences = pd.DataFrame(sentence_pairs)\n",
        "print(\"Created a DataFrame with example sentence pairs:\")\n",
        "print(df_sentences.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ca2794"
      },
      "source": [
        "## Select and Load a Pre-trained Language Model\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable pre-trained transformer-based language model (e.g., T5, BART, Pegasus) that is effective for text-to-text generation tasks like paraphrasing. Load the model and its tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5804023b"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to import the necessary libraries, choose a pre-trained model for text-to-text generation, and then load its tokenizer and model as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca6f2c38"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 2. Select a suitable pre-trained model\n",
        "model_name = \"t5-small\"\n",
        "\n",
        "# 3. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Loaded tokenizer for {model_name}\")\n",
        "\n",
        "# 4. Load the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "print(f\"Loaded model {model_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d1d230d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully loaded the tokenizer and model, despite a warning that did not prevent the execution. The next logical step is to prepare the example sentence pairs for fine-tuning by tokenizing them and creating a dataset in a format suitable for the pre-trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83484469"
      },
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"paraphrase: {text}\" for text in examples[\"original\"]]\n",
        "    targets = examples[\"formal\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
        "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(df_sentences)\n",
        "\n",
        "# Apply the preprocessing function\n",
        "tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "print(\"Original dataset structure:\")\n",
        "print(hf_dataset)\n",
        "print(\"\\nTokenized dataset structure:\")\n",
        "print(tokenized_dataset)\n",
        "print(\"\\nExample of tokenized input_ids and labels:\")\n",
        "print(tokenized_dataset[0][\"input_ids\"])\n",
        "print(tokenized_dataset[0][\"labels\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64f9c6ec"
      },
      "source": [
        "## Fine-tune the Model for Paraphrasing\n",
        "\n",
        "### Subtask:\n",
        "Fine-tune the selected pre-trained model using the prepared dataset of example sentence pairs. This step will adapt the model to generate paraphrases according to the desired 'clearer and more formal' style.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6e159dc"
      },
      "source": [
        "**Reasoning**:\n",
        "To fine-tune the model, I need to import the necessary `TrainingArguments` and `Trainer` classes, define the training parameters, instantiate a `DataCollatorForSeq2Seq` for dynamic padding, and then create a `Trainer` object to commence the fine-tuning process by calling its `train()` method. All these steps are sequential and interdependent, therefore they will be included in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bfa78d1"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# 2. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\", # Evaluate every epoch\n",
        "    save_strategy=\"epoch\",       # Save checkpoint every epoch\n",
        "    logging_dir='./logs',        # Directory for storing logs\n",
        "    logging_steps=10,            # Log every 10 steps\n",
        "    report_to=[\"none\"]           # Disable report to W&B or other services\n",
        ")\n",
        "\n",
        "# 3. Import DataCollatorForSeq2Seq and instantiate it\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# 4. Instantiate the Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset, # Using training set for evaluation as per instructions for simplicity\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting model fine-tuning...\")\n",
        "# 5. Call the .train() method\n",
        "trainer.train()\n",
        "print(\"Model fine-tuning complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42103fec"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `TrainingArguments` received an unexpected keyword argument `evaluation_strategy`. This indicates an older version of the `transformers` library. To fix this, I will remove both `evaluation_strategy` and `save_strategy` parameters, as they likely are not supported in this version, allowing the training to proceed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3a5d02c"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# 2. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    # evaluation_strategy=\"epoch\", # Removed due to TypeError with older transformers versions\n",
        "    # save_strategy=\"epoch\",       # Removed due to TypeError with older transformers versions\n",
        "    logging_dir='./logs',        # Directory for storing logs\n",
        "    logging_steps=10,            # Log every 10 steps\n",
        "    report_to=[\"none\"]           # Disable report to W&B or other services\n",
        ")\n",
        "\n",
        "# 3. Import DataCollatorForSeq2Seq and instantiate it\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# 4. Instantiate the Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset, # Using training set for evaluation as per instructions for simplicity\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting model fine-tuning...\")\n",
        "# 5. Call the .train() method\n",
        "trainer.train()\n",
        "print(\"Model fine-tuning complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cc0e086"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully completed the model fine-tuning process, as indicated by the 'Model fine-tuning complete' message in the standard output. The warnings encountered were non-critical and did not prevent the training from finishing. The next logical step, as per the overall task, is to evaluate the fine-tuned model's performance on new, unseen sentences to confirm its ability to paraphrase in a clearer and more formal style.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3373f1e6"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the fine-tuned model's ability to generate clearer and more formal paraphrases by testing it with new sentences and comparing the output to expected formal versions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b58ff28e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to introduce the evaluation subtask with a markdown-formatted string in a `code_block` (which implies a `python` cell type) resulted in a `SyntaxError`. The model training has completed, and the next logical step is to implement the evaluation. I will now provide a Python `code_block` that directly addresses the evaluation subtask by defining a function to generate paraphrases using the fine-tuned model and testing it with new example sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "238429f5"
      },
      "source": [
        "def paraphrase_sentence(sentence, model, tokenizer, max_length=128):\n",
        "    # Preprocess the input sentence\n",
        "    input_text = f\"paraphrase: {sentence}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
        "\n",
        "    # Generate the paraphrase\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=max_length,\n",
        "        num_beams=5,  # Use beam search for better quality\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    paraphrase = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return paraphrase\n",
        "\n",
        "# Test with new sentences\n",
        "test_sentences = [\n",
        "    \"I need to talk to you ASAP about the stuff.\",\n",
        "    \"Let's get this done today, okay?\",\n",
        "    \"He totally messed up the presentation.\",\n",
        "    \"Could you quickly check this for me?\",\n",
        "    \"I'm gonna be late for the meeting.\"\n",
        "]\n",
        "\n",
        "print(\"Evaluating model performance with new sentences:\\n\")\n",
        "for original_sentence in test_sentences:\n",
        "    formal_paraphrase = paraphrase_sentence(original_sentence, model, tokenizer)\n",
        "    print(f\"Original: {original_sentence}\")\n",
        "    print(f\"Formal:   {formal_paraphrase}\\n\")\n",
        "\n",
        "print(\"Evaluation complete. Please manually inspect the generated paraphrases for clarity and formality.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddaa9856"
      },
      "source": [
        "## Demonstrate Paraphrasing\n",
        "\n",
        "### Subtask:\n",
        "Provide examples of how to use the implemented paraphrasing function, showcasing its ability to transform sentences from less formal to more formal and clear, similar to the 'Students must submit the report before Friday' example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12e4fad0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous paraphrasing results indicated issues such as repeating prefixes, lack of actual paraphrasing, or incorrect language generation. To address these, I will modify the `paraphrase_sentence` function by experimenting with `model.generate()` parameters, specifically increasing `num_beams` for a wider search and adding `max_new_tokens` to control the length of the generated output more effectively, as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c03d531"
      },
      "source": [
        "def paraphrase_sentence(sentence, model, tokenizer, max_input_length=128, max_new_tokens=50):\n",
        "    # Preprocess the input sentence, ensuring it matches the training format\n",
        "    input_text = f\"paraphrase: {sentence}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Generate the paraphrase with refined parameters\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        num_beams=10,  # Increased beam search for potentially better quality\n",
        "        max_new_tokens=max_new_tokens, # Control the length of the generated output\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2 # Discourage repetition of n-grams\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    paraphrase = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return paraphrase\n",
        "\n",
        "# Test with the same new sentences\n",
        "test_sentences = [\n",
        "    \"I need to talk to you ASAP about the stuff.\",\n",
        "    \"Let's get this done today, okay?\",\n",
        "    \"He totally messed up the presentation.\",\n",
        "    \"Could you quickly check this for me?\",\n",
        "    \"I'm gonna be late for the meeting.\"\n",
        "]\n",
        "\n",
        "print(\"Evaluating model performance with new sentences after refinement:\\n\")\n",
        "for original_sentence in test_sentences:\n",
        "    formal_paraphrase = paraphrase_sentence(original_sentence, model, tokenizer)\n",
        "    print(f\"Original: {original_sentence}\")\n",
        "    print(f\"Formal:   {formal_paraphrase}\\n\")\n",
        "\n",
        "print(\"Evaluation complete. Please manually inspect the generated paraphrases for clarity and formality and assess if critical errors are resolved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf234884"
      },
      "source": [
        "### Analysis of Paraphrasing Results and Conclusion\n",
        "\n",
        "The most recent attempt to paraphrase sentences with the fine-tuned `t5-small` model, even after adjusting generation parameters (`num_beams=10`, `max_new_tokens=50`, `no_repeat_ngram_size=2`), still yields unsatisfactory results.\n",
        "\n",
        "**Observed Issues:**\n",
        "*   **Repetitive Prefixes:** The output often includes the `Paraphrase:` prefix, indicating that the model is struggling to correctly interpret or apply the instruction, potentially due to the `paraphrase: {sentence}` input format combined with the limited training data.\n",
        "*   **Lack of Transformation:** Many sentences are returned with little to no actual paraphrasing, or only minor changes that do not achieve the desired clarity and formality (e.g., \"He totally messed up the presentation.\" remains unchanged).\n",
        "*   **Incorrect Output:** In some instances, the model generates completely erroneous text, such as \"False\", which is a critical failure in generating coherent English.\n",
        "\n",
        "**Reason for Limitations:**\n",
        "As anticipated, the primary limitation is the extremely small training dataset (10 example pairs). A language model of this complexity requires a significantly larger and more diverse set of examples to effectively learn the nuanced rules of stylistic paraphrasing, including formal vocabulary, sentence structure changes, and clarity enhancements. The `t5-small` model is powerful, but its ability to generalize from such limited data is inherently restricted. It mostly memorizes the input-output pairs or struggles to extract meaningful patterns beyond simple prompt repetition.\n",
        "\n",
        "**Conclusion for this Subtask:**\n",
        "While the `paraphrase_sentence` function has been implemented and refined to the best extent possible given the constraints, the model's performance in transforming sentences into genuinely clearer and more formal versions is currently very limited. Critical errors in output generation persist. For this subtask of demonstrating the function, we have shown what it *can* do with the current setup. To achieve the broader goal of an effective AI paraphraser, future steps would necessitate:\n",
        "1.  **Significantly expanding the training dataset** with many more diverse and high-quality informal-to-formal sentence pairs.\n",
        "2.  Potentially exploring a **larger pre-trained model** that has a greater capacity to learn complex linguistic transformations.\n",
        "3.  Further **experimenting with prompt engineering** and fine-tuning strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f2683a2"
      },
      "source": [
        "### Analysis of Paraphrasing Results and Conclusion\n",
        "\n",
        "The most recent attempt to paraphrase sentences with the fine-tuned `t5-small` model, even after adjusting generation parameters (`num_beams=10`, `max_new_tokens=50`, `no_repeat_ngram_size=2`), still yields unsatisfactory results.\n",
        "\n",
        "**Observed Issues:**\n",
        "*   **Repetitive Prefixes:** The output often includes the `Paraphrase:` prefix, indicating that the model is struggling to correctly interpret or apply the instruction, potentially due to the `paraphrase: {sentence}` input format combined with the limited training data.\n",
        "*   **Lack of Transformation:** Many sentences are returned with little to no actual paraphrasing, or only minor changes that do not achieve the desired clarity and formality (e.g., \"He totally messed up the presentation.\" remains unchanged).\n",
        "*   **Incorrect Output:** In some instances, the model generates completely erroneous text, such as \"False\", which is a critical failure in generating coherent English.\n",
        "\n",
        "**Reason for Limitations:**\n",
        "As anticipated, the primary limitation is the extremely small training dataset (10 example pairs). A language model of this complexity requires a significantly larger and more diverse set of examples to effectively learn the nuanced rules of stylistic paraphrasing, including formal vocabulary, sentence structure changes, and clarity enhancements. The `t5-small` model is powerful, but its ability to generalize from such limited data is inherently restricted. It mostly memorizes the input-output pairs or struggles to extract meaningful patterns beyond simple prompt repetition.\n",
        "\n",
        "**Conclusion for this Subtask:**\n",
        "While the `paraphrase_sentence` function has been implemented and refined to the best extent possible given the constraints, the model's performance in transforming sentences into genuinely clearer and more formal versions is currently very limited. Critical errors in output generation persist. For this subtask of demonstrating the function, we have shown what it *can* do with the current setup. To achieve the broader goal of an effective AI paraphraser, future steps would necessitate:\n",
        "1.  **Significantly expanding the training dataset** with many more diverse and high-quality informal-to-formal sentence pairs.\n",
        "2.  Potentially exploring a **larger pre-trained model** that has a greater capacity to learn complex linguistic transformations.\n",
        "3.  Further **experimenting with prompt engineering** and fine-tuning strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53caca71"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Present the completed AI paraphraser, including the model, code, and a demonstration of its functionality to transform sentences into clearer and more formal English.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ded283e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The task was to develop an AI paraphraser that transforms English sentences into clearer and more formal versions.\n",
        "\n",
        "The AI paraphraser was developed using the `t5-small` model and its tokenizer, fine-tuned on a small custom dataset of 10 sentence pairs. While a function to paraphrase was implemented and demonstrated, the model's performance in transforming sentences into genuinely clearer and more formal versions was very limited. The model exhibited issues such as repetitive prefixes, a lack of meaningful transformation, and occasionally incorrect or nonsensical outputs (e.g., \"False\", German phrases).\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Paraphrasing Requirements Clarification**: The initial step successfully outlined instructions for identifying characteristics of clearer and more formal English, including analyzing examples, brainstorming formal language patterns (e.g., 'get' vs. 'obtain', avoiding contractions, using passive voice), and focusing on precise word choice. However, it provided the *guidelines* for clarification rather than the clarified rules themselves.\n",
        "*   **Dataset Creation**: A dataset of 10 example sentence pairs was successfully created and structured into a pandas DataFrame. Each pair consisted of an 'original' (informal/less clear) sentence and its 'formal' (clearer/more formal) counterpart. Examples include transforming \"I gotta go now, fam.\" to \"I must depart at this moment.\"\n",
        "*   **Model Selection and Preparation**: The `t5-small` pre-trained transformer model and its corresponding tokenizer were successfully loaded. The created dataset was then preprocessed and tokenized, formatted as required for T5 fine-tuning (e.g., prepending \"paraphrase: \" to inputs).\n",
        "*   **Model Fine-tuning Challenges**: Initial attempts to fine-tune the model encountered `TypeError` due to incompatible arguments with the `transformers` library version. After resolving these, the `t5-small` model was successfully fine-tuned over 3 epochs.\n",
        "*   **Paraphraser Performance**: The fine-tuned model's paraphrasing capability was significantly limited. Despite adjustments to generation parameters like `num_beams` (increased to 10), `max_new_tokens` (set to 50), and `no_repeat_ngram_size` (set to 2), the model produced unsatisfactory results.\n",
        "    *   Outputs frequently included repetitive prefixes (e.g., \"Paraphrase:\").\n",
        "    *   Many sentences showed little to no actual transformation or improvement in clarity/formality (e.g., \"He totally messed up the presentation.\" remained unchanged).\n",
        "    *   Some generated outputs were erroneous or incoherent (e.g., \"False\" or German phrases for English inputs).\n",
        "*   **Primary Limitation**: The key reason for the poor performance was identified as the extremely small training dataset (10 example pairs), which was insufficient for the `t5-small` model to learn complex stylistic paraphrasing rules.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   To achieve effective formal paraphrasing, a significantly larger and more diverse dataset of informal-to-formal sentence pairs is crucial. The current dataset of 10 examples is insufficient for the `t5-small` model to generalize adequately.\n",
        "*   Further exploration into advanced prompt engineering techniques or considering a larger pre-trained language model might also be beneficial once a more robust dataset is available, to better capture nuanced linguistic transformations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76f529e4"
      },
      "source": [
        "# Task\n",
        "Load and inspect the `/content/Grammar Correction.csv` dataset to determine its suitability for training an AI paraphraser that transforms sentences from less formal to more formal and clear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5f1ab9"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the `/content/Grammar Correction.csv` file into a pandas DataFrame to inspect its content and structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b779bb3"
      },
      "source": [
        "**Reasoning**:\n",
        "The user explicitly asked to load the CSV file into a pandas DataFrame and store it as `df_grammar`. I will import pandas, read the CSV, and then display the head of the DataFrame to show its content and structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65266ebd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_grammar = pd.read_csv('/content/Grammar Correction.csv')\n",
        "\n",
        "print(\"DataFrame 'df_grammar' loaded successfully. Displaying the first 5 rows:\")\n",
        "print(df_grammar.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5f1c684"
      },
      "source": [
        "## Inspect Dataset\n",
        "\n",
        "### Subtask:\n",
        "Display the first few rows of the DataFrame, its columns, and general information to understand the data format and types. This will help determine if it contains original and formal sentence pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db47a572"
      },
      "source": [
        "**Reasoning**:\n",
        "I will display the column names, general information, shape, and missing values of the `df_grammar` DataFrame to understand its structure and content, as requested by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f68ac5c"
      },
      "source": [
        "print(\"Column Names:\")\n",
        "print(df_grammar.columns.tolist())\n",
        "\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df_grammar.info()\n",
        "\n",
        "print(\"\\nDataFrame Shape (rows, columns):\")\n",
        "print(df_grammar.shape)\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_grammar.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8eeea8c"
      },
      "source": [
        "## Analyze Dataset Content\n",
        "\n",
        "### Subtask:\n",
        "Based on the inspection, analyze if the dataset's content aligns with the task's requirement of transforming sentences from less formal to more formal and clear. We will look for columns that could represent original and formal versions of sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a1aaf57"
      },
      "source": [
        "**Reasoning**:\n",
        "To analyze a diverse set of rows as instructed, I will sample 10 random rows from the `df_grammar` DataFrame and display them. This will allow for a broader understanding of the transformations beyond just the initial rows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ea842bb"
      },
      "source": [
        "print(\"\\nSample of 10 random rows from df_grammar for analysis:\")\n",
        "print(df_grammar.sample(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7463fdb0"
      },
      "source": [
        "### Analysis of Dataset Content for Paraphrasing Suitability\n",
        "\n",
        "Upon examining the 'Ungrammatical Statement' and 'Standard English' columns in the `df_grammar` DataFrame, particularly across the sampled rows and the initial entries, the following observations can be made:\n",
        "\n",
        "1.  **Nature of Transformations**: The primary focus of the transformations in this dataset is **grammar correction**. The 'Standard English' column consistently corrects grammatical errors found in the 'Ungrammatical Statement' column. These errors include verb tense (e.g., \"I goes\" to \"I go\"), subject-verb agreement (e.g., \"They was\" to \"They were\"), spelling (e.g., \"played\" to \"play\" in past continuous context, assuming a typo), and correct usage of articles, prepositions, and contractions.\n",
        "\n",
        "2.  **Formal vs. Grammatical**: While grammatical correctness is a component of clear and formal language, this dataset does not appear to primarily target a *stylistic shift* from less formal to more formal language. Instead, it corrects errors that would make a statement incorrect regardless of its formality level.\n",
        "    *   For example, \"He didn't eat no breakfast this morning.\" (Ungrammatical) becomes \"He didn't eat any breakfast this morning.\" (Standard English). This is a correction of a double negative, not a change in formality.\n",
        "    *   \"I been waiting for the bus for over an hour.\" (Ungrammatical) becomes \"I have been waiting for the bus for over an hour.\" (Standard English). This corrects a common informal grammatical omission, making it grammatically correct, but not necessarily more formal in the stylistic sense.\n",
        "    *   Sentences like \"The car is parked in front of the house.\" and its corrected version \"The car is parked in front the house.\" show subtle grammatical corrections rather than a conscious effort to elevate formality (e.g., from 'in front of' to 'prior to' if that was the intent).\n",
        "\n",
        "3.  **Lack of Broader Stylistic Changes**: There is little evidence of transformations involving:\n",
        "    *   **Vocabulary Enhancement**: Replacing informal words with more sophisticated or formal synonyms (e.g., 'get' with 'obtain', 'stuff' with 'materials/items').\n",
        "    *   **Sentence Structure Refinement**: Rephrasing colloquialisms or simplifying complex but grammatically correct informal sentences into more concise or formal structures.\n",
        "    *   **Tone Adjustment**: Shifting from a casual tone to a professional or academic one.\n",
        "\n",
        "**Conclusion on Suitability**:\n",
        "The `/content/Grammar Correction.csv` dataset is highly suitable for training a model to perform **grammar correction**. However, it is **not directly suitable** for the broader task of transforming sentences from 'less formal to more formal and clear' if 'formal' implies significant stylistic and lexical changes beyond mere grammatical rectifications. The dataset primarily addresses correctness rather than formality or clarity as distinct stylistic objectives. While a grammatically correct sentence is clearer than an incorrect one, the dataset does not provide examples of sentences that are *grammatically correct but informal* being transformed into *grammatically correct and formal* versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d70521c8"
      },
      "source": [
        "### Analysis of Dataset Content for Paraphrasing Suitability\n",
        "\n",
        "Upon examining the 'Ungrammatical Statement' and 'Standard English' columns in the `df_grammar` DataFrame, particularly across the sampled rows and the initial entries, the following observations can be made:\n",
        "\n",
        "1.  **Nature of Transformations**: The primary focus of the transformations in this dataset is **grammar correction**. The 'Standard English' column consistently corrects grammatical errors found in the 'Ungrammatical Statement' column. These errors include verb tense (e.g., \"I goes\" to \"I go\"), subject-verb agreement (e.g., \"They was\" to \"They were\"), spelling (e.g., \"played\" to \"play\" in past continuous context, assuming a typo), and correct usage of articles, prepositions, and contractions.\n",
        "\n",
        "2.  **Formal vs. Grammatical**: While grammatical correctness is a component of clear and formal language, this dataset does not appear to primarily target a *stylistic shift* from less formal to more formal language. Instead, it corrects errors that would make a statement incorrect regardless of its formality level.\n",
        "    *   For example, \"He didn't eat no breakfast this morning.\" (Ungrammatical) becomes \"He didn't eat any breakfast this morning.\" (Standard English). This is a correction of a double negative, not a change in formality.\n",
        "    *   \"I been waiting for the bus for over an hour.\" (Ungrammatical) becomes \"I have been waiting for the bus for over an hour.\" (Standard English). This corrects a common informal grammatical omission, making it grammatically correct, but not necessarily more formal in the stylistic sense.\n",
        "    *   Sentences like \"The car is parked in front of the house.\" and its corrected version \"The car is parked in front the house.\" show subtle grammatical corrections rather than a conscious effort to elevate formality (e.g., from 'in front of' to 'prior to' if that was the intent).\n",
        "\n",
        "3.  **Lack of Broader Stylistic Changes**: There is little evidence of transformations involving:\n",
        "    *   **Vocabulary Enhancement**: Replacing informal words with more sophisticated or formal synonyms (e.g., 'get' with 'obtain', 'stuff' with 'materials/items').\n",
        "    *   **Sentence Structure Refinement**: Rephrasing colloquialisms or simplifying complex but grammatically correct informal sentences into more concise or formal structures.\n",
        "    *   **Tone Adjustment**: Shifting from a casual tone to a professional or academic one.\n",
        "\n",
        "**Conclusion on Suitability**:\n",
        "The `/content/Grammar Correction.csv` dataset is highly suitable for training a model to perform **grammar correction**. However, it is **not directly suitable** for the broader task of transforming sentences from 'less formal to more formal and clear' if 'formal' implies significant stylistic and lexical changes beyond mere grammatical rectifications. The dataset primarily addresses correctness rather than formality or clarity as distinct stylistic objectives. While a grammatically correct sentence is clearer than an incorrect one, the dataset does not provide examples of sentences that are *grammatically correct but informal* being transformed into *grammatically correct and formal* versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa13fddd"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a conclusion on whether the `/content/Grammar Correction.csv` dataset is suitable for training the AI paraphraser, along with any recommendations for its use or potential modifications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5cb75a0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The `/content/Grammar Correction.csv` dataset is **not directly suitable** for the broader task of training an AI paraphraser that transforms sentences from \"less formal to more formal and clear\" if 'formal' implies significant stylistic, lexical, or structural changes beyond basic grammatical rectification. While it is highly suitable for training a model specifically for grammar correction, it primarily addresses correctness rather than stylistic formality or clarity as distinct objectives.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The dataset was successfully loaded into a pandas DataFrame named `df_grammar`, containing 2018 rows and 4 columns.\n",
        "*   Key columns include 'Ungrammatical Statement' and 'Standard English', which are both of `object` type (strings), indicating text data.\n",
        "*   There are no missing values across any of the columns in the DataFrame.\n",
        "*   The transformations observed in the dataset are almost exclusively **grammar corrections**, such as fixing verb tense (e.g., \"I goes\" to \"I go\"), subject-verb agreement (e.g., \"They was\" to \"They were\"), double negatives (e.g., \"He didn't eat no breakfast\" to \"He didn't eat any breakfast\"), and proper usage of articles/prepositions.\n",
        "*   The dataset lacks evidence of broader stylistic changes required for a 'less formal to more formal' transformation, such as vocabulary enhancement, sentence structure refinement for formality, or tone adjustment.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   If the goal is to train an AI paraphraser for stylistic formality, a different dataset focusing on varying formality levels of grammatically correct sentences would be required.\n",
        "*   The current dataset is valuable for building a robust grammar correction module, which could be a prerequisite step in a larger formalization pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4641252d"
      },
      "source": [
        "# Task\n",
        "Create an AI that takes an ungrammatical English sentence and transforms it into a grammatically correct version, using the provided `/content/Grammar Correction.csv` dataset for training and demonstration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eb642ac"
      },
      "source": [
        "## Define New Task: Grammar Correction AI\n",
        "\n",
        "### Subtask:\n",
        "Clearly define the objective of the new AI as transforming English sentences into grammatically correct versions using the provided dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50d87235"
      },
      "source": [
        "## Define New Task: Grammar Correction AI\n",
        "\n",
        "### Subtask:\n",
        "Clearly define the objective of the new AI as transforming English sentences into grammatically correct versions using the provided dataset.\n",
        "\n",
        "#### Objective Definition:\n",
        "1.  **Primary Objective**: To develop an Artificial Intelligence system capable of correcting grammatical errors in English sentences.\n",
        "2.  **Input**: The AI will accept English sentences that may contain various grammatical inaccuracies.\n",
        "3.  **Output**: The AI will produce grammatically correct versions of the input sentences, aiming for standard English usage.\n",
        "4.  **Dataset for this Task**: The model will leverage the `/content/Grammar Correction.csv` dataset for training and validation, which contains pairs of ungrammatical and standard English sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa18a41c"
      },
      "source": [
        "## Prepare Grammar Correction Dataset\n",
        "\n",
        "### Subtask:\n",
        "Tokenize and format the 'Ungrammatical Statement' and 'Standard English' columns from the `df_grammar` DataFrame into a Hugging Face Dataset, suitable for fine-tuning a sequence-to-sequence model. This ensures the data is ready for the new task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c42b71"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the `Dataset` class, convert the `df_grammar` DataFrame to a Hugging Face Dataset, define a preprocessing function to tokenize and format the input and target columns for grammar correction, apply this function to create a tokenized dataset, and then print the structures and an example of the tokenized data as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3666b5c5"
      },
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_grammar_function(examples):\n",
        "    inputs = [f\"grammar correction: {text}\" for text in examples[\"Ungrammatical Statement\"]]\n",
        "    targets = examples[\"Standard English\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
        "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# 1. Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "hf_grammar_dataset = Dataset.from_pandas(df_grammar)\n",
        "\n",
        "# 3. Apply the preprocessing function\n",
        "tokenized_grammar_dataset = hf_grammar_dataset.map(preprocess_grammar_function, batched=True)\n",
        "\n",
        "# 4. Print the structure of both datasets and an example\n",
        "print(\"Original grammar dataset structure:\")\n",
        "print(hf_grammar_dataset)\n",
        "print(\"\\nTokenized grammar dataset structure:\")\n",
        "print(tokenized_grammar_dataset)\n",
        "print(\"\\nExample of tokenized input_ids and labels from grammar dataset:\")\n",
        "print(tokenized_grammar_dataset[0][\"input_ids\"])\n",
        "print(tokenized_grammar_dataset[0][\"labels\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "302ae710"
      },
      "source": [
        "## Select and Load Pre-trained Model (T5)\n",
        "\n",
        "### Subtask:\n",
        "Re-load a clean instance of the t5-small model and its tokenizer specifically for this new grammar correction task. This avoids any interference or learned biases from the previous paraphrasing fine-tuning, ensuring a fresh start.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468103fd"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-import the necessary classes and load a fresh instance of the t5-small tokenizer and model to ensure no previous fine-tuning influences the new grammar correction task, as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "150785ca"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 2. Set the model name\n",
        "model_name = \"t5-small\"\n",
        "\n",
        "# 3. Load a new instance of the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Re-loaded tokenizer for {model_name}\")\n",
        "\n",
        "# 4. Load a new instance of the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "print(f\"Re-loaded model {model_name} for grammar correction task\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b10b843"
      },
      "source": [
        "## Fine-tune Model for Grammar Correction\n",
        "\n",
        "### Subtask:\n",
        "Fine-tune this new instance of the pre-trained model using the prepared grammar correction dataset. This will adapt the model specifically for identifying and correcting grammatical errors, as per the new mission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c9bdfcc"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now fine-tune the newly loaded `t5-small` model for grammar correction using the `tokenized_grammar_dataset`. This involves importing necessary classes, defining training arguments, instantiating a data collator, and then creating and running the `Trainer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99e8294"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# 2. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./grammar_results\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./grammar_logs',\n",
        "    logging_steps=10,\n",
        "    report_to=[\"none\"]\n",
        ")\n",
        "\n",
        "# 3. Instantiate DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# 4. Instantiate the Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_grammar_dataset,\n",
        "    eval_dataset=tokenized_grammar_dataset, # Using training set for evaluation for simplicity\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting model fine-tuning for grammar correction...\")\n",
        "# 5. Call the .train() method\n",
        "trainer.train()\n",
        "print(\"Model fine-tuning for grammar correction complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8979ff4d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully fine-tuned the model for grammar correction. The next step is to evaluate the fine-tuned model's ability to correct new ungrammatical sentences and demonstrate its functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b160676e"
      },
      "source": [
        "def correct_grammar_sentence(sentence, model, tokenizer, max_input_length=128, max_new_tokens=128):\n",
        "    # Preprocess the input sentence, ensuring it matches the training format\n",
        "    input_text = f\"grammar correction: {sentence}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Generate the corrected output\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        num_beams=5,  # Use beam search for better quality\n",
        "        max_new_tokens=max_new_tokens, # Control the length of the generated output\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2 # Discourage repetition of n-grams\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_sentence\n",
        "\n",
        "# Test with new ungrammatical sentences\n",
        "test_ungrammatical_sentences = [\n",
        "    \"He don't know the answer.\",\n",
        "    \"Me and him is going to the store.\",\n",
        "    \"She have a lot of books.\",\n",
        "    \"The dog run fastly.\",\n",
        "    \"I seen that movie yesterday.\"\n",
        "]\n",
        "\n",
        "print(\"Evaluating grammar correction model performance with new sentences:\\n\")\n",
        "for original_sentence in test_ungrammatical_sentences:\n",
        "    corrected_sentence = correct_grammar_sentence(original_sentence, model, tokenizer)\n",
        "    print(f\"Original (Ungrammatical): {original_sentence}\")\n",
        "    print(f\"Corrected (Grammatical):    {corrected_sentence}\\n\")\n",
        "\n",
        "print(\"Evaluation complete. Please manually inspect the generated sentences for grammatical correctness.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aee0c6db"
      },
      "source": [
        "## Demonstrate Grammar Correction\n",
        "\n",
        "### Subtask:\n",
        "Provide examples of the implemented grammar correction function in action, showcasing its ability to transform ungrammatical sentences into standard English. This will explicitly demonstrate the success of the new AI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bc44ea"
      },
      "source": [
        "### Subtask\n",
        "Provide examples of the implemented grammar correction function in action, showcasing its ability to transform ungrammatical sentences into standard English. This will explicitly demonstrate the success of the new AI.\n",
        "\n",
        "#### Instructions\n",
        "1. Utilize the already defined `correct_grammar_sentence` function.\n",
        "2. Create a diverse list of new ungrammatical sentences to serve as test cases, ensuring they cover various types of grammatical errors (e.g., verb tense, subject-verb agreement, pronoun usage, common informal constructions).\n",
        "3. Iterate through this list of test sentences.\n",
        "4. For each sentence, call the `correct_grammar_sentence` function to obtain its grammatically corrected version.\n",
        "5. Print both the original ungrammatical sentence and its corrected counterpart in a clear, comparative format.\n",
        "6. Include a concluding statement prompting a manual review of the corrections to assess the model's performance and highlight its strengths and limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d80c18a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement the instructions to demonstrate the grammar correction function. This involves defining new ungrammatical test sentences, iterating through them, applying the `correct_grammar_sentence` function, and printing the original and corrected versions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36605791"
      },
      "source": [
        "print(\"\\n--- Demonstrating Grammar Correction AI ---\\n\")\n",
        "\n",
        "# 2. Create a diverse list of new ungrammatical sentences\n",
        "demonstration_sentences = [\n",
        "    \"He go to the store everyday.\",\n",
        "    \"The cat and dog is sleeping.\",\n",
        "    \"Her and me will finish the report.\",\n",
        "    \"They was happy yesterday.\",\n",
        "    \"I should of went there.\",\n",
        "    \"Everyone are excited about the news.\",\n",
        "    \"She don't like coffee.\",\n",
        "    \"The more faster, the better.\"\n",
        "]\n",
        "\n",
        "# 3. Iterate through the list of test sentences and 4. Call the correct_grammar_sentence function\n",
        "for i, original_sentence in enumerate(demonstration_sentences):\n",
        "    corrected_sentence = correct_grammar_sentence(original_sentence, model, tokenizer)\n",
        "\n",
        "    # 5. Print both the original ungrammatical sentence and its corrected counterpart\n",
        "    print(f\"Demonstration {i+1}:\")\n",
        "    print(f\"  Original (Ungrammatical): {original_sentence}\")\n",
        "    print(f\"  Corrected (Grammatical):    {corrected_sentence}\\n\")\n",
        "\n",
        "# 6. Include a concluding statement\n",
        "print(\"--- Demonstration Complete ---\")\n",
        "print(\"Please manually review the generated corrections to assess the model's performance,\")\n",
        "print(\"noting both its successes in grammar correction and any remaining limitations.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92faca2d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Present the completed Grammar Correction AI, including the model, code, and a demonstration of its functionality to make English sentences grammatically correct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dd97a5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Dataset Preparation**: The provided `/content/Grammar Correction.csv` dataset was successfully converted into a Hugging Face Dataset and preprocessed. This involved tokenizing 'Ungrammatical Statement' (prefixed with \"grammar correction: \") and 'Standard English' columns into `input_ids` and `labels`, respectively, using a `max_length` of 128 for both.\n",
        "*   **Model Selection**: A new, clean instance of the `t5-small` model and its corresponding tokenizer was loaded, ensuring no prior biases from other tasks influenced the grammar correction training.\n",
        "*   **Model Fine-tuning**: The `t5-small` model was successfully fine-tuned on the grammar correction dataset.\n",
        "*   **Demonstrated Performance**: The fine-tuned model showed partial success in correcting grammatical errors.\n",
        "    *   **Successful Corrections**: It correctly handled subject-verb agreement in cases like \"The cat and dog **is** sleeping.\" to \"The cat and dog **are** sleeping.\" and \"Everyone **are** excited...\" to \"Everyone **is** excited...\", corrected verb tense in \"They **was** happy...\" to \"They **were** happy...\", and improved modal verb usage in \"I **should of went** there.\" to \"I **should have gone** there.\"\n",
        "    *   **Areas for Improvement**: The model struggled with certain common errors, such as changing \"He **go** to the store\" to \"He **goes** to the store,\" correcting \"She **don't** like coffee\" to \"She **doesn't** like coffee,\" or handling pronoun cases like \"**Her and me** will finish the report.\" It also generated an incorrect output for \"The more faster, the better.\" (producing \"The more faster faster, the better.\").\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current `t5-small` model, even after fine-tuning, exhibits limitations in handling a diverse range of grammatical errors. A next step could be to explore larger or more specialized pre-trained models (e.g., T5-base or T5-large) that might have a stronger foundation for complex linguistic tasks.\n",
        "*   To address the specific errors the model failed to correct, consider augmenting the training dataset with more examples targeting those error types (e.g., subject-verb agreement with pronouns, negative constructions, comparative adjectives). Alternatively, implement error-specific fine-tuning or rule-based post-processing for common unhandled cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d741d988"
      },
      "source": [
        "## Tools Used\n",
        "\n",
        "To develop both the paraphrasing and grammar correction AIs, we utilized the following key tools and libraries:\n",
        "\n",
        "*   **pandas**: A powerful data manipulation and analysis library for Python, used for handling DataFrames and structuring our datasets (e.g., `df_sentences`, `df_grammar`).\n",
        "*   **Hugging Face `datasets` library**: Used for creating and managing datasets in a format optimized for machine learning models, particularly `Dataset.from_pandas` and `map` for preprocessing.\n",
        "*   **Hugging Face `transformers` library**: The core library for working with pre-trained transformer models.\n",
        "    *   `AutoTokenizer` and `AutoModelForSeq2SeqLM`: For loading the T5 (`t5-small`) model and its tokenizer.\n",
        "    *   `TrainingArguments`: For configuring the training parameters during fine-tuning.\n",
        "    *   `Trainer`: A high-level API for simplifying the training loop of Hugging Face models.\n",
        "    *   `DataCollatorForSeq2Seq`: For preparing batches of data, including dynamic padding, suitable for sequence-to-sequence models."
      ]
    }
  ]
}